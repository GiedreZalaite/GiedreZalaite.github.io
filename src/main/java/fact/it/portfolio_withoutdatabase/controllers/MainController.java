package fact.it.portfolio_withoutdatabase.controllers;

import fact.it.portfolio_withoutdatabase.models.Project;
import jakarta.annotation.PostConstruct;
import jakarta.servlet.http.HttpServletRequest;
import org.springframework.stereotype.Controller;
import org.springframework.ui.Model;
import org.springframework.web.bind.annotation.RequestMapping;

import java.util.ArrayList;

@Controller
public class MainController {

    private ArrayList<Project> projects = new ArrayList<>();
    @PostConstruct
    private void fillData(){
        projects.add(new Project("Strawberry Detection App", "Project 4.0","2022-2023", "Group", "For this project we developed an app and web application for the company VITO, enabling users to capture photos on their phones and automatically count the number of strawberries. <br> The app stores data, generates graphs, and provides management features for fields, workers, and more. <br> Additionally, the web application accepts high-resolution drone images of fields, accurately counting the number of flowers present. <br> <br> Project encompassed three key phases. <br> The first two phases spanned an entire semester and involved requirement gathering, preparation, concept development, and prototyping. <br> The final phase lasted less than four weeks, during which we implemented the application and delivered a presentation.",
                "First of all, I labelled all flowers in the provided images. <br> I enhanced our dataset through data augmentation techniques, increasing picture quantity. <br> For precise identification of small objects like flowers, I divided images into a 10x10 grid.  <br> <br> Initial model was developed using Detectron2, which is based on the Faster R-CNN architecture and is well-suited for analyzing drone imagery. <br> However, we faced challenges related to library compatibility when deploying. To address these challenges and ensure seamless exportability, I decided to adjust the model using the YOLOv7 framework. This framework provided efficient object detection capabilities and allowed us to tailor the model to the specific needs of our project.  <br><br>Within the final drone model, I carefully adjusted the confidence threshold to classify detected objects as flowers only if the confidence rate exceeded a predetermined percentage. <br> I also implemented a granular approach that allowed for slight variations in the number of flowers detected in different portions of the 10x10 grid. <br> These methods aimed to ensure precise identification of the total number of flowers while maintaining consistent confidence levels across all detected objects.",
                "We had a team of 6 members, including 2 APP students, 2 CCS students, and 2 AI students.  <br> <br> In the first 2 phases each team member actively contributed, with a slight emphasis on our respective areas of expertise. <br> During the 2nd phase I assumed the role of team leader, ensuring timely deliverables and overseeing the progress. <br> <br> In the final phase as AI students, we took charge of creating 2 AI models and developing an API to integrate them into the application. <br> I was responsible for the drone image model, and everything associated with it, for example implementing a slicing pipeline that enhanced accuracy by processing image parts individually.",
                "After testing the model with various images, an accuracy rate of around 94% was achieved.  <br> The final model was exported to the Roboflow server, where it can be used as an API call in future projects. <br> <br> To implement the model in a web application, API functions were created using Express.js. <br> However, due to time constraints, the APP team was unable to complete this integration, and the model remained in the API testing environment. <br> <br> We ended up focusing on phone image model that was made using YOLOv5, then transferred to TensorFlow lite for easier deployment and faster load. <br> This model was successfully implemented in both applications. <br> <br> The mobile app allowed users to login, manage field owners, add farms and fields, capture pictures with flower detection, and visualize data on maps and statistics. <br> The web app provided similar features for admins to manage field owners, farms, and workers. <br> An API was developed to facilitate database interactions and support data retrieval, storage, and editing. <br> <br> Overall, our project delivered user-friendly applications for efficient farm and flower data management.",
                "/images/project4.0_bgr.jpg",
                "/images/project4.0_img1.jpg",
                "/images/project4.0_img2.jpg"));
        projects.add(new Project("Cheese Classifier", "Big Data", "2022", "Group", "The main requirement for the project was to make a model that would classify 5 different types of chosen object and implement the model in a website. <br> <br> We decided to classify 5 different types of cheese. We ended up with 4 different models: fastAI, Keras, Google teachable machine and Vertex AI of Google Cloud. <br> We compared these models, implemented them in Streamlit application and added extra possibility in the application to summarize pasted text using NLP model. <br> The application also allows a user to explore dataset by entering a number and type of cheese, and re-train one of the models on the application itself with selected fastAI options.",
                        ":  Google Teachable Machine is a web-based tool that doesnâ€™t require a lot of knowledge which makes it a good base benchmark to build on top of. <br> <br> As for Google Vertex AI, I opted for a straightforward model. <br>  I uploaded images for each category and employed AutoML for training. To ensure optimal performance, I adjusted various options such as maximum node hours and type. <br> Initially, I trained the model with lower parameters to assess accuracy and determine the need for retraining. However, the accuracy proved to be satisfactory, eliminating the need for further training. <br> <br> To create the Streamlit application, I began by importing the necessary libraries and models. Subsequently, I wrote code for the home page and proceeded to implement each model on separate pages. <br>  Additionally, I developed a page dedicated to training the model and exploring the dataset. All of these application components were written in Python. <br> <br> Lastly, I utilized a pretrained transformer-based model from HuggingFace. <br> This model was fine-tuned and retrained specifically for the purpose of summarizing text input by users. To achieve this, I integrated user input, initialized a pipeline to access the model, and generated the desired output. <br> The entire process was implemented within the Streamlit framework. <br>  To benchmark our results, we also pasted the same story into ChatGPT for comparison.",
                        "Our team consisted of three AI students. <br> <br> My main role was to prepare Google's Teachable Machine and Vertex AI from Google Cloud for comparison and implementation within Streamlit. <br> I successfully developed the Streamlit application, integrated all the models, and incorporated features to explore data and retrain the fastAI model. <br> Additionally, I underwent training in a transformer-based NLP model to summarize text, and I also integrated it into Streamlit for practical use.",
                        "We gathered a diverse dataset of cheese images using a Python scraper.  <br> <br> Through our experimentation, we discovered that the Keras library yielded the best results among the models we tested with 93% accuracy. Leveraging transfer learning and advanced training options, this model achieved exceptional accuracy and speed. <br> Close behind (92.4% accuracy), the FastAI model also demonstrated impressive performance by utilizing transfer learning and fine-tuning techniques. <br> <br> While other models, such as those trained using Google Vertex (91.5% accuracy) or Google's Teachable Machine (80% accuracy), didn't reach the same level of performance as Keras or FastAI, they still offered unique advantages. <br> For instance, the Google models boasted a more compact structure, making them easier to deploy in cloud environments. They also featured simpler training processes. <br> <br> In addition to the model training, we successfully accomplished various other project tasks. These included implementing the Keras model, incorporating ROC and AUC metrics, integrating Google Teachable Machine and Vertex, developing the ability to train a model on Streamlit, and finally, creating an NLP model capable of text summarization. <br> When we compared our NLP model to summarization of ChatGPT, we noticed that while 2 models differentiate in details, our model gives quite a decent summary. <br> <br> Overall this project was a lot of hard work but provided us with a lot of experience and knowledge in using different types or models and how to deploy it to a production environment.",
                        "/images/bigdata_bgr.jpg",
                        "/images/bigdata_img1.jpg", "/images/bigdata_img2.jpg"));
        projects.add(new Project("Titanic & Crops Group Assignment", "Data Science", "2022", "Group","In this project, our main task was to create visualizations for two distinct datasets: the Titanic dataset obtained from Kaggle and the Crop statistics dataset spanning from 1900 to 2017, sourced from NASA. <br> <br> Using R we had to delve into the data, perform data exploration, and carry out necessary data cleansing procedures. <br> <br> Subsequently, we utilized our analytical insights to determine the most effective visualizations that would effectively communicate the narratives encapsulated within our datasets.",
                "In the assignment, I focused on exploring the Titanic dataset from Kaggle. I undertook data cleaning, particularly addressing missing values. <br> I replaced missing ages based on average values corresponding to specific titles (e.g., Mr, Miss, Master). <br> I replaced missing values in the \"embarked\" column with the most common value, 'S' and so on. Additionally, I performed various data manipulations such as creating new tables, implementing if statements, and utilizing group by operations. <br> <br> Subsequently, I formulated visualization-driven questions and selected suitable graphs to delve into the main predictors of survival. <br> By layering different factors, such as class, title, age, sex, and family size, I sought to uncover correlations and influences on survival. <br> My primary objective was to examine the impact of economic factors (passenger class and ticket fare), family size, gender, and age on survival rates. <br> Through six graphs, including bar charts, scatter plots, and combined graphs, I investigated the relationship between age, gender, title, class, ticket fare, family size, and survival, aiming to reveal meaningful correlations. <br> <br> <br> Regarding the crop statistics dataset, I addressed missing values by inferring them based on related columns or simply replacing them with empty values for unused columns.<br>  Furthermore, I eliminated remaining missing values as they did not affect the storyline. <br> Specifically, my contribution to this dataset involved creating box plots with outliers and percentiles to highlight the crucial measure of yield. <br> Additionally, I generated a multi-color graph illustrating the production of crops over the years, differentiating crops by color.",
                "As part of a team of three, I played a pivotal role in overseeing and coordinating our project. I ensured that the work was properly documented and uploaded. <br> <br> With some help of a teammate, I worked on cleaning up both datasets. <br> Additionally, I took the lead in working on the Titanic dataset making most of the graphs, while also contributing to the creation and execution of some graphs for the crop dataset.",
                "Based on the six visualizations we created for the Titanic dataset, it became evident that women had a significantly higher chance of survival compared to men. <br> Male children had a greater likelihood of survival than adult men, while unmarried (young) women had similar survival rates to married women. <br> <br> However, the most influential predictor for survival appeared to be the passenger class. <br> Approximately three out of four individuals in the first-class survived the disaster, whereas only one out of four individuals in the third-class were able to survive. <br> Women in the first and second classes generally had a good chance of survival, while women in the third class experienced higher mortality rates, except for children and individuals over 60 years old, who had relatively better survival rates. <br> Among men, those in the first class had a significantly higher chance of survival compared to those in the second and third classes. <br> <br> Additionally, the size of one's family had a minor influence on survival. Passengers with a small family (1-3 members) had a higher chance of survival, while those with no family also had a slightly lower chance. In contrast, passengers with large families did not survive, possibly due to the increased responsibilities and efforts to save everyone. <br> <br> <br> Regarding the crop statistics dataset, based on the eight graphs we analyzed, we observed a consistent increase in the production and yield of crops over time. <br> Particularly, the yield of maize and the production of maize and cereals demonstrated a significant increase, indicating improved efficiency in maize production. While other crops also exhibited an increase, the growth was comparatively less dramatic. <br> Furthermore, the analysis revealed prominent crop production centers, such as the United States and China, which displayed substantial production levels.",
                "/images/datascience_bgr.jpg",
                "/images/datascience_img2.jpg", "/images/datascience_img1.jpg"));

    }
    @RequestMapping("/aboutme")
    public String aboutme(){
        return "aboutme";
    }
    @RequestMapping("/internship")
    public String internship(){
        return "internship";
    }
    @RequestMapping("/projects")
    public String projects(Model model){
        model.addAttribute("projects", projects);
        return "projects";
    }
    @RequestMapping("/projectdetails")
    public String projectdetails(HttpServletRequest request, Model model){
        int projectIndex = Integer.parseInt(request.getParameter("projectIndex"));
        Project returnProject = projects.get(projectIndex);
        model.addAttribute("project", returnProject);
        return "projectdetails";
    }
}
